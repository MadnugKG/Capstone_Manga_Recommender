{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ed5b7d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20be5dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "from lightfm import LightFM, cross_validation\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.evaluation import precision_at_k, auc_score, recall_at_k\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae3d59",
   "metadata": {},
   "source": [
    "### Functions for Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f941c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_evaluation(recommended_set, read_set):\n",
    "    match_length = len(read_set.intersection(recommended_set))\n",
    "    if match_length:\n",
    "        return [True, len(read_set), match_length]\n",
    "    else:\n",
    "        return [False, len(read_set), match_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae570f",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24355d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (3,4,9,89) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data_cleaned/data_merged.csv', keep_default_na=False, na_values=['-', '', ' '])\n",
    "manga_titles = pd.read_csv('../data_cleaned/manga_titles_cleaned.csv')\n",
    "manga_titles.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b776f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['manga_length'] = data['manga_length'].astype(str)\n",
    "data['clusters'] = data['clusters'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3b36bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1434060 entries, 0 to 1434059\n",
      "Data columns (total 100 columns):\n",
      " #   Column                     Non-Null Count    Dtype  \n",
      "---  ------                     --------------    -----  \n",
      " 0   index                      1434060 non-null  int64  \n",
      " 1   manga_title                1434060 non-null  object \n",
      " 2   manga_link                 1434060 non-null  object \n",
      " 3   volumes                    1434060 non-null  object \n",
      " 4   chapters                   1434060 non-null  object \n",
      " 5   publication_status         1434060 non-null  object \n",
      " 6   published                  1434060 non-null  object \n",
      " 7   serialization              1434060 non-null  object \n",
      " 8   authors                    1434060 non-null  object \n",
      " 9   published_year_start       1434060 non-null  object \n",
      " 10  genres_Adventure           1434060 non-null  float64\n",
      " 11  genres_Action              1434060 non-null  float64\n",
      " 12  genres_Horror              1434060 non-null  float64\n",
      " 13  genres_Drama               1434060 non-null  float64\n",
      " 14  genres_Fantasy             1434060 non-null  float64\n",
      " 15  genres_Award Winning       1434060 non-null  float64\n",
      " 16  genres_Supernatural        1434060 non-null  float64\n",
      " 17  genres_Mystery             1434060 non-null  float64\n",
      " 18  genres_Sports              1434060 non-null  float64\n",
      " 19  genres_Slice of Life       1434060 non-null  float64\n",
      " 20  genres_Comedy              1434060 non-null  float64\n",
      " 21  genres_Sci-Fi              1434060 non-null  float64\n",
      " 22  genres_Ecchi               1434060 non-null  float64\n",
      " 23  genres_Romance             1434060 non-null  float64\n",
      " 24  genres_Suspense            1434060 non-null  float64\n",
      " 25  genres_Girls Love          1434060 non-null  float64\n",
      " 26  genres_Boys Love           1434060 non-null  float64\n",
      " 27  genres_Gourmet             1434060 non-null  float64\n",
      " 28  genres_Not Specified       1434060 non-null  float64\n",
      " 29  genres_Avant Garde         1434060 non-null  float64\n",
      " 30  genres_Erotica             1434060 non-null  float64\n",
      " 31  themes_Gore                1434060 non-null  float64\n",
      " 32  themes_Mythology           1434060 non-null  float64\n",
      " 33  themes_Military            1434060 non-null  float64\n",
      " 34  themes_Psychological       1434060 non-null  float64\n",
      " 35  themes_Historical          1434060 non-null  float64\n",
      " 36  themes_Not Specified       1434060 non-null  float64\n",
      " 37  themes_Samurai             1434060 non-null  float64\n",
      " 38  themes_Adult Cast          1434060 non-null  float64\n",
      " 39  themes_Team Sports         1434060 non-null  float64\n",
      " 40  themes_School              1434060 non-null  float64\n",
      " 41  themes_Combat Sports       1434060 non-null  float64\n",
      " 42  themes_Anthropomorphic     1434060 non-null  float64\n",
      " 43  themes_Iyashikei           1434060 non-null  float64\n",
      " 44  themes_Childcare           1434060 non-null  float64\n",
      " 45  themes_Strategy Game       1434060 non-null  float64\n",
      " 46  themes_Romantic Subtext    1434060 non-null  float64\n",
      " 47  themes_Workplace           1434060 non-null  float64\n",
      " 48  themes_Delinquents         1434060 non-null  float64\n",
      " 49  themes_Space               1434060 non-null  float64\n",
      " 50  themes_Love Polygon        1434060 non-null  float64\n",
      " 51  themes_Music               1434060 non-null  float64\n",
      " 52  themes_Super Power         1434060 non-null  float64\n",
      " 53  themes_Parody              1434060 non-null  float64\n",
      " 54  themes_Visual Arts         1434060 non-null  float64\n",
      " 55  themes_Performing Arts     1434060 non-null  float64\n",
      " 56  themes_Gag Humor           1434060 non-null  float64\n",
      " 57  themes_Mecha               1434060 non-null  float64\n",
      " 58  themes_Showbiz             1434060 non-null  float64\n",
      " 59  themes_Survival            1434060 non-null  float64\n",
      " 60  themes_Memoir              1434060 non-null  float64\n",
      " 61  themes_Otaku Culture       1434060 non-null  float64\n",
      " 62  themes_Vampire             1434060 non-null  float64\n",
      " 63  themes_Reincarnation       1434060 non-null  float64\n",
      " 64  themes_Reverse Harem       1434060 non-null  float64\n",
      " 65  themes_Crossdressing       1434060 non-null  float64\n",
      " 66  themes_High Stakes Game    1434060 non-null  float64\n",
      " 67  themes_Organized Crime     1434060 non-null  float64\n",
      " 68  themes_Harem               1434060 non-null  float64\n",
      " 69  themes_Martial Arts        1434060 non-null  float64\n",
      " 70  themes_Time Travel         1434060 non-null  float64\n",
      " 71  themes_Medical             1434060 non-null  float64\n",
      " 72  themes_Detective           1434060 non-null  float64\n",
      " 73  themes_Isekai              1434060 non-null  float64\n",
      " 74  themes_Mahou Shoujo        1434060 non-null  float64\n",
      " 75  themes_CGDCT               1434060 non-null  float64\n",
      " 76  themes_Pets                1434060 non-null  float64\n",
      " 77  themes_Racing              1434060 non-null  float64\n",
      " 78  themes_Magical Sex Shift   1434060 non-null  float64\n",
      " 79  themes_Video Game          1434060 non-null  float64\n",
      " 80  themes_Villainess          1434060 non-null  float64\n",
      " 81  themes_Educational         1434060 non-null  float64\n",
      " 82  themes_Idols (Male)        1434060 non-null  float64\n",
      " 83  demographic_Seinen         1434060 non-null  float64\n",
      " 84  demographic_Shounen        1434060 non-null  float64\n",
      " 85  demographic_Not Specified  1434060 non-null  float64\n",
      " 86  demographic_Shoujo         1434060 non-null  float64\n",
      " 87  demographic_Josei          1434060 non-null  float64\n",
      " 88  demographic_Kids           1434060 non-null  float64\n",
      " 89  published_decade_start     1434060 non-null  object \n",
      " 90  member                     1434060 non-null  object \n",
      " 91  score                      1434060 non-null  float64\n",
      " 92  read_status                1434060 non-null  object \n",
      " 93  member_link                1434060 non-null  object \n",
      " 94  last_vol                   1434060 non-null  float64\n",
      " 95  last_chpt                  1434060 non-null  float64\n",
      " 96  max_content_read           1434060 non-null  float64\n",
      " 97  title_avg_score            1434060 non-null  float64\n",
      " 98  manga_length               1434060 non-null  object \n",
      " 99  clusters                   1434060 non-null  object \n",
      "dtypes: float64(84), int64(1), object(15)\n",
      "memory usage: 1.1+ GB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b4bc2",
   "metadata": {},
   "source": [
    "### Train Test Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f8fd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of titles each member read\n",
    "temp = data.groupby('member')['manga_title'].count()\n",
    "\n",
    "# Filter away those member who read less than 5 books\n",
    "X = data[data['member'].isin(temp[temp>=34].index)]\n",
    "X_remain = data[data['member'].isin(temp[temp<34].index)]\n",
    "\n",
    "# Split the dataset into train and test\n",
    "data_train, data_test = train_test_split(X, stratify=X['member'], train_size=0.7, random_state=42)\n",
    "\n",
    "# Adding X_remain back to X\n",
    "data_train = pd.concat((data_train, X_remain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a586697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1362482, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of X_train\n",
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53757d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71578, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of X_test\n",
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1200e937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05253500596705131"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of dataset used for testing\n",
    "data_test.shape[0]/data_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909868ae",
   "metadata": {},
   "source": [
    "### Using LightFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a42839f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_no = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db1e6910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to track overall results\n",
    "results={}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cf543",
   "metadata": {},
   "source": [
    "### Basic Model With warp as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2815344",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = None\n",
    "result_name = 'basic_model_warp'\n",
    "\n",
    "# A dict in dict to store the results\n",
    "results[result_name] = {}\n",
    "\n",
    "# To create mapping\n",
    "lfm_dataset = Dataset(user_identity_features=False, item_identity_features=True)\n",
    "\n",
    "# Fit the dataset\n",
    "lfm_dataset.fit(data_train['member'].unique(), data_train['index'].unique(), item_features=features_list)\n",
    "\n",
    "# To create train_interaction and train_weights\n",
    "temp = [(data_train.loc[i, 'member'], data_train.loc[i, 'index'], data_train.loc[i, 'score']) for i in data_train.index]\n",
    "train_interactions, train_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# To create test_interaction and test_weights\n",
    "temp = [(data_test.loc[i, 'member'], data_test.loc[i, 'index'], data_test.loc[i, 'score']) for i in data_test.index]\n",
    "test_interactions, test_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# Create item features\n",
    "item_features = features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c355ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_threads = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "487dbd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████| 50/50 [03:40<00:00,  4.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train model using warp as loss function\n",
    "model = LightFM(k=10, learning_rate=0.05, loss='warp', random_state=state_no)\n",
    "model.fit(train_interactions, sample_weight=train_weights, epochs=50, num_threads=no_of_threads, verbose=True)\n",
    "pickle.dump(model, open('../data_production/basic_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd67e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision of the model using the train dataset\n",
    "train_precision = precision_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_precision\n",
    "results[result_name]['train_precision@k'] = train_precision.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_precision = precision_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_precision@k'] = test_precision.mean()\n",
    "\n",
    "# Calculate the recall of the model using the train dataset\n",
    "train_recall = recall_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_recall\n",
    "results[result_name]['train_recall@k'] = train_recall.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_recall = recall_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_recall@k'] = test_recall.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the train dataset\n",
    "train_auc = auc_score(model, train_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_auc\n",
    "results[result_name]['train_auc'] = train_auc.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the test dataset\n",
    "test_auc = auc_score(model, test_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_auc\n",
    "results[result_name]['test_auc'] = test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc9a90c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basic_model_warp': {'train_precision@k': 0.13202193,\n",
       "  'test_precision@k': 0.18290076,\n",
       "  'train_recall@k': 0.7006574748139289,\n",
       "  'test_recall@k': 0.11001033957753074,\n",
       "  'train_auc': 0.9887221,\n",
       "  'test_auc': 0.889645}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ded005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "# Evaluate model using train dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_train['member'].unique()[0:10000]\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_train['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    scoring_df = scoring_df.sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['train accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7e67063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_lfm        0.26660\n",
       "no_titles_read    7.15720\n",
       "no_match          0.45860\n",
       "precision@k       0.04586\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_train_lfm_result = lfm_result\n",
    "basic_train_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64f0d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heavy_readers</th>\n",
       "      <td>0.862069</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light_reader</th>\n",
       "      <td>0.247671</td>\n",
       "      <td>9662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderate_reader</th>\n",
       "      <td>0.788845</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean  count\n",
       "groupings                       \n",
       "heavy_readers    0.862069     87\n",
       "light_reader     0.247671   9662\n",
       "moderate_reader  0.788845    251"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = basic_train_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1654271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_test['member'].unique()\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    train_read_list = data_train[data_train['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_test['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    unread_list = [i for i in scoring_df.index if i not in train_read_list]\n",
    "    scoring_df = scoring_df.loc[unread_list,:].sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['test accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090e5e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basic_model_warp': {'train_precision@k': 0.13202193,\n",
       "  'test_precision@k': 0.18290076,\n",
       "  'train_recall@k': 0.7006574748139289,\n",
       "  'test_recall@k': 0.11001033957753074,\n",
       "  'train_auc': 0.9887221,\n",
       "  'test_auc': 0.889645,\n",
       "  'train accuracy': 0.2666,\n",
       "  'test accuracy': 0.7768447837150128}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1c059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_test_lfm_result = lfm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fed37e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_lfm         0.776845\n",
       "no_titles_read    18.211705\n",
       "no_match           1.828753\n",
       "precision@k        0.182875\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_test_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c8e02e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heavy_readers</th>\n",
       "      <td>0.965517</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light_reader</th>\n",
       "      <td>0.764198</td>\n",
       "      <td>3592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderate_reader</th>\n",
       "      <td>0.892430</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean  count\n",
       "groupings                       \n",
       "heavy_readers    0.965517     87\n",
       "light_reader     0.764198   3592\n",
       "moderate_reader  0.892430    251"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = basic_test_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83febbfa",
   "metadata": {},
   "source": [
    "### Modeling with Feature Set 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f1230e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_details = pd.read_csv(\"../data_cleaned/manga_details_cleaned.csv\")\n",
    "genres_list = [col for col in manga_details.columns if 'genres' in col]\n",
    "features_list = genres_list\n",
    "result_name = 'feature1_model_warp'\n",
    "\n",
    "# A dict in dict to store the results\n",
    "results[result_name] = {}\n",
    "\n",
    "# To create mapping\n",
    "lfm_dataset = Dataset(user_identity_features=False, item_identity_features=True)\n",
    "\n",
    "# Fit the dataset\n",
    "lfm_dataset.fit(data_train['member'].unique(), data_train['index'].unique(), item_features=features_list)\n",
    "\n",
    "# To create train_interaction and train_weights\n",
    "temp = [(data_train.loc[i, 'member'], data_train.loc[i, 'index'], data_train.loc[i, 'score']) for i in data_train.index]\n",
    "train_interactions, train_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# To create test_interaction and test_weights\n",
    "temp = [(data_test.loc[i, 'member'], data_test.loc[i, 'index'], data_test.loc[i, 'score']) for i in data_test.index]\n",
    "test_interactions, test_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# Create item features\n",
    "manga_details.set_index('index', inplace=True)\n",
    "manga_details = manga_details.loc[data_train['index'].unique()]\n",
    "temp = (manga_details.loc[:, features_list].to_dict(orient='index'))\n",
    "item_feature_scores = zip(temp.keys(), temp.values())\n",
    "item_features = lfm_dataset.build_item_features(item_feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f761441",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_threads = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "669e8e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████| 50/50 [12:58<00:00, 15.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train model using warp as loss function\n",
    "model = LightFM(learning_rate=0.05, loss='warp', random_state=state_no)\n",
    "model.fit(train_interactions, item_features=item_features, sample_weight=train_weights, epochs=50, num_threads=no_of_threads, verbose=True)\n",
    "pickle.dump(model, open('../data_production/model1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09a4f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision of the model using the train dataset\n",
    "train_precision = precision_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_precision\n",
    "results[result_name]['train_precision@k'] = train_precision.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_precision = precision_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_precision@k'] = test_precision.mean()\n",
    "\n",
    "# Calculate the recall of the model using the train dataset\n",
    "train_recall = recall_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_recall\n",
    "results[result_name]['train_recall@k'] = train_recall.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_recall = recall_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_recall@k'] = test_recall.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the train dataset\n",
    "train_auc = auc_score(model, train_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_auc\n",
    "results[result_name]['train_auc'] = train_auc.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the test dataset\n",
    "test_auc = auc_score(model, test_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_auc\n",
    "results[result_name]['test_auc'] = test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f08789ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basic_model_warp': {'train_precision@k': 0.13202193,\n",
       "  'test_precision@k': 0.18290076,\n",
       "  'train_recall@k': 0.7006574748139289,\n",
       "  'test_recall@k': 0.11001033957753074,\n",
       "  'train_auc': 0.9887221,\n",
       "  'test_auc': 0.889645,\n",
       "  'train accuracy': 0.2666,\n",
       "  'test accuracy': 0.7768447837150128},\n",
       " 'feature1_model_warp': {'train_precision@k': 0.10263066,\n",
       "  'test_precision@k': 0.13417304,\n",
       "  'train_recall@k': 0.5799568389921415,\n",
       "  'test_recall@k': 0.0803872673839875,\n",
       "  'train_auc': 0.98273766,\n",
       "  'test_auc': 0.859975}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b3a1bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "# Evaluate model using train dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_train['member'].unique()[0:10000]\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_train['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    scoring_df = scoring_df.sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['train accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b9af633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_lfm        0.14410\n",
       "no_titles_read    7.15720\n",
       "no_match          0.18890\n",
       "precision@k       0.01889\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_train_lfm_result = lfm_result\n",
    "model1_train_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d105dfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heavy_readers</th>\n",
       "      <td>0.747126</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light_reader</th>\n",
       "      <td>0.126268</td>\n",
       "      <td>9662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderate_reader</th>\n",
       "      <td>0.621514</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean  count\n",
       "groupings                       \n",
       "heavy_readers    0.747126     87\n",
       "light_reader     0.126268   9662\n",
       "moderate_reader  0.621514    251"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = model1_train_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb2c7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r"
     ]
    }
   ],
   "source": [
    "# Evaluate model using test dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_test['member'].unique()\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    train_read_list = data_train[data_train['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_test['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    unread_list = [i for i in scoring_df.index if i not in train_read_list]\n",
    "    scoring_df = scoring_df.loc[unread_list,:].sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['test accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97b2d2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basic_model_warp': {'train_precision@k': 0.13202193,\n",
       "  'test_precision@k': 0.18290076,\n",
       "  'train_recall@k': 0.7006574748139289,\n",
       "  'test_recall@k': 0.11001033957753074,\n",
       "  'train_auc': 0.9887221,\n",
       "  'test_auc': 0.889645,\n",
       "  'train accuracy': 0.2666,\n",
       "  'test accuracy': 0.7768447837150128},\n",
       " 'feature1_model_warp': {'train_precision@k': 0.10263066,\n",
       "  'test_precision@k': 0.13417304,\n",
       "  'train_recall@k': 0.5799568389921415,\n",
       "  'test_recall@k': 0.0803872673839875,\n",
       "  'train_auc': 0.98273766,\n",
       "  'test_auc': 0.859975,\n",
       "  'train accuracy': 0.1441,\n",
       "  'test accuracy': 0.39974554707379134}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f2d67eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_test_lfm_result = lfm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61c76838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>groupings</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>heavy_readers</th>\n",
       "      <td>0.827586</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>light_reader</th>\n",
       "      <td>0.370267</td>\n",
       "      <td>3592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderate_reader</th>\n",
       "      <td>0.673307</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     mean  count\n",
       "groupings                       \n",
       "heavy_readers    0.827586     87\n",
       "light_reader     0.370267   3592\n",
       "moderate_reader  0.673307    251"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = model1_test_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c7a57c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_lfm         0.399746\n",
       "no_titles_read    18.211705\n",
       "no_match           0.565903\n",
       "precision@k        0.056590\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_test_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc9c35",
   "metadata": {},
   "source": [
    "### Modeling with Feature Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6313a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_details = pd.read_csv(\"../data_cleaned/manga_details_cleaned.csv\")\n",
    "themes_list = [col for col in manga_details.columns if 'themes' in col]\n",
    "features_list = themes_list\n",
    "result_name = 'feature2_model_warp'\n",
    "\n",
    "# A dict in dict to store the results\n",
    "results[result_name] = {}\n",
    "\n",
    "# To create mapping\n",
    "lfm_dataset = Dataset(user_identity_features=False, item_identity_features=True)\n",
    "\n",
    "# Fit the dataset\n",
    "lfm_dataset.fit(data_train['member'].unique(), data_train['index'].unique(), item_features=features_list)\n",
    "\n",
    "# To create train_interaction and train_weights\n",
    "temp = [(data_train.loc[i, 'member'], data_train.loc[i, 'index'], data_train.loc[i, 'score']) for i in data_train.index]\n",
    "train_interactions, train_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# To create test_interaction and test_weights\n",
    "temp = [(data_test.loc[i, 'member'], data_test.loc[i, 'index'], data_test.loc[i, 'score']) for i in data_test.index]\n",
    "test_interactions, test_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# Create item features\n",
    "manga_details.set_index('index', inplace=True)\n",
    "manga_details = manga_details.loc[data_train['index'].unique()]\n",
    "temp = (manga_details.loc[:, features_list].to_dict(orient='index'))\n",
    "item_feature_scores = zip(temp.keys(), temp.values())\n",
    "item_features = lfm_dataset.build_item_features(item_feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c07e596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_threads = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eadd5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  42%|███████████████████████████████▌                                           | 21/50 [18:07<22:54, 47.39s/it]"
     ]
    }
   ],
   "source": [
    "# Train model using warp as loss function\n",
    "model = LightFM(learning_rate=0.05, loss='warp', random_state=state_no)\n",
    "model.fit(train_interactions, item_features=item_features, sample_weight=train_weights, epochs=50, num_threads=no_of_threads, verbose=True)\n",
    "pickle.dump(model, open('../data_production/model2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision of the model using the train dataset\n",
    "train_precision = precision_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_precision\n",
    "results[result_name]['train_precision@k'] = train_precision.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_precision = precision_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_precision@k'] = test_precision.mean()\n",
    "\n",
    "# Calculate the recall of the model using the train dataset\n",
    "train_recall = recall_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_recall\n",
    "results[result_name]['train_recall@k'] = train_recall.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_recall = recall_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_recall@k'] = test_recall.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the train dataset\n",
    "train_auc = auc_score(model, train_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_auc\n",
    "results[result_name]['train_auc'] = train_auc.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the test dataset\n",
    "test_auc = auc_score(model, test_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_auc\n",
    "results[result_name]['test_auc'] = test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa82720",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using train dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_train['member'].unique()[0:10000]\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_train['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    scoring_df = scoring_df.sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['train accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_train_lfm_result = lfm_result\n",
    "model2_train_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = basic_train_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633be859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using test dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_test['member'].unique()\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    train_read_list = data_train[data_train['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_test['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    unread_list = [i for i in scoring_df.index if i not in train_read_list]\n",
    "    scoring_df = scoring_df.loc[unread_list,:].sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['test accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9824599",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3df9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_test_lfm_result = lfm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = model2_test_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae92f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_test_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139bbadd",
   "metadata": {},
   "source": [
    "### Modeling Using Feature Set 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fc289",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_details = pd.read_csv(\"../data_cleaned/manga_details_cleaned.csv\")\n",
    "demographic_list = [col for col in manga_details.columns if 'demographic' in col]\n",
    "features_list = demographic_list\n",
    "result_name = 'feature3_model_warp'\n",
    "\n",
    "# A dict in dict to store the results\n",
    "results[result_name] = {}\n",
    "\n",
    "# To create mapping\n",
    "lfm_dataset = Dataset(user_identity_features=False, item_identity_features=True)\n",
    "\n",
    "# Fit the dataset\n",
    "lfm_dataset.fit(data_train['member'].unique(), data_train['index'].unique(), item_features=features_list)\n",
    "\n",
    "# To create train_interaction and train_weights\n",
    "temp = [(data_train.loc[i, 'member'], data_train.loc[i, 'index'], data_train.loc[i, 'score']) for i in data_train.index]\n",
    "train_interactions, train_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# To create test_interaction and test_weights\n",
    "temp = [(data_test.loc[i, 'member'], data_test.loc[i, 'index'], data_test.loc[i, 'score']) for i in data_test.index]\n",
    "test_interactions, test_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# Create item features\n",
    "manga_details.set_index('index', inplace=True)\n",
    "manga_details = manga_details.loc[data_train['index'].unique()]\n",
    "temp = (manga_details.loc[:, features_list].to_dict(orient='index'))\n",
    "item_feature_scores = zip(temp.keys(), temp.values())\n",
    "item_features = lfm_dataset.build_item_features(item_feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_threads = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using warp as loss function\n",
    "model = LightFM(learning_rate=0.05, loss='warp', random_state=state_no)\n",
    "model.fit(train_interactions, item_features=item_features, sample_weight=train_weights, epochs=50, num_threads=no_of_threads, verbose=True)\n",
    "pickle.dump(model, open('../data_production/model3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision of the model using the train dataset\n",
    "train_precision = precision_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_precision\n",
    "results[result_name]['train_precision@k'] = train_precision.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_precision = precision_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_precision@k'] = test_precision.mean()\n",
    "\n",
    "# Calculate the recall of the model using the train dataset\n",
    "train_recall = recall_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_recall\n",
    "results[result_name]['train_recall@k'] = train_recall.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_recall = recall_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_recall@k'] = test_recall.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the train dataset\n",
    "train_auc = auc_score(model, train_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_auc\n",
    "results[result_name]['train_auc'] = train_auc.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the test dataset\n",
    "test_auc = auc_score(model, test_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_auc\n",
    "results[result_name]['test_auc'] = test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de786feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using train dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_train['member'].unique()[0:10000]\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_train['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    scoring_df = scoring_df.sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['train accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c88f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_train_lfm_result = lfm_result\n",
    "model3_train_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = basic_train_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using test dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_test['member'].unique()\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    train_read_list = data_train[data_train['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_test['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    unread_list = [i for i in scoring_df.index if i not in train_read_list]\n",
    "    scoring_df = scoring_df.loc[unread_list,:].sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['test accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07951d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_test_lfm_result = lfm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_test_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce95467",
   "metadata": {},
   "source": [
    "### Modeling Using Feature Set 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4529e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "manga_details = pd.read_csv(\"../data_cleaned/manga_details_cleaned.csv\")\n",
    "features_list = genres_list + themes_list + demographic_list\n",
    "result_name = 'feature4_model_warp'\n",
    "\n",
    "# A dict in dict to store the results\n",
    "results[result_name] = {}\n",
    "\n",
    "# To create mapping\n",
    "lfm_dataset = Dataset(user_identity_features=False, item_identity_features=True)\n",
    "\n",
    "# Fit the dataset\n",
    "lfm_dataset.fit(data_train['member'].unique(), data_train['index'].unique(), item_features=features_list)\n",
    "\n",
    "# To create train_interaction and train_weights\n",
    "temp = [(data_train.loc[i, 'member'], data_train.loc[i, 'index'], data_train.loc[i, 'score']) for i in data_train.index]\n",
    "train_interactions, train_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# To create test_interaction and test_weights\n",
    "temp = [(data_test.loc[i, 'member'], data_test.loc[i, 'index'], data_test.loc[i, 'score']) for i in data_test.index]\n",
    "test_interactions, test_weights = lfm_dataset.build_interactions(temp)\n",
    "\n",
    "# Create item features\n",
    "manga_details.set_index('index', inplace=True)\n",
    "manga_details = manga_details.loc[data_train['index'].unique()]\n",
    "temp = (manga_details.loc[:, features_list].to_dict(orient='index'))\n",
    "item_feature_scores = zip(temp.keys(), temp.values())\n",
    "item_features = lfm_dataset.build_item_features(item_feature_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f64e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_threads = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c8225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using warp as loss function\n",
    "model = LightFM(learning_rate=0.05, loss='warp', random_state=state_no)\n",
    "model.fit(train_interactions, item_features=item_features, sample_weight=train_weights, epochs=50, num_threads=no_of_threads, verbose=True)\n",
    "pickle.dump(model, open('../data_production/model4.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision of the model using the train dataset\n",
    "train_precision = precision_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_precision\n",
    "results[result_name]['train_precision@k'] = train_precision.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_precision = precision_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_precision@k'] = test_precision.mean()\n",
    "\n",
    "# Calculate the recall of the model using the train dataset\n",
    "train_recall = recall_at_k(model, train_interactions, item_features=item_features, k=10, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_recall\n",
    "results[result_name]['train_recall@k'] = train_recall.mean()\n",
    "\n",
    "# Calculate the precision of the model using the test dataset\n",
    "test_recall = recall_at_k(model, test_interactions, train_interactions=train_interactions, item_features=item_features, k=10, check_intersections=False, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_precision\n",
    "results[result_name]['test_recall@k'] = test_recall.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the train dataset\n",
    "train_auc = auc_score(model, train_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the train_auc\n",
    "results[result_name]['train_auc'] = train_auc.mean()\n",
    "\n",
    "# Calculate the auc score of the model using the test dataset\n",
    "test_auc = auc_score(model, test_interactions, item_features=item_features, num_threads=no_of_threads)\n",
    "# Calculate the overall mean of the test_auc\n",
    "results[result_name]['test_auc'] = test_auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe766e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63771241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using train dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_train['member'].unique()[0:10000]\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_train['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    scoring_df = scoring_df.sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['train accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1375ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_train_lfm_result = lfm_result\n",
    "model4_train_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grouping based on number of titles read\n",
    "result = basic_train_lfm_result\n",
    "result['groupings'] = result['no_titles_read'].apply(lambda x: 'heavy_readers' if x > 50 else 'moderate_reader' if x > 30 else 'light_reader')\n",
    "\n",
    "# Calculate the mean Accuracy of Each Groups\n",
    "result.groupby('groupings')['result_lfm'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df076974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using test dataset\n",
    "evaluation_result = {}\n",
    "count = 0\n",
    "top_k = 10\n",
    "\n",
    "member_list = data_test['member'].unique()\n",
    "for user in member_list:\n",
    "    read_list = data_test[data_test['member']==user]['index'].values\n",
    "    train_read_list = data_train[data_train['member']==user]['index'].values\n",
    "    \n",
    "    user = lfm_dataset.mapping()[0][user]\n",
    "    scoring_df = pd.DataFrame(model.predict(user_ids=user, item_ids=np.arange(data_test['index'].nunique())))\n",
    "    scoring_df = scoring_df.merge(pd.DataFrame(lfm_dataset.mapping()[2].keys()), left_index=True, right_index=True)\n",
    "    scoring_df.columns = ['score', 'title']\n",
    "    scoring_df.set_index('title', inplace=True)\n",
    "    scoring_df.columns = ['score']\n",
    "    unread_list = [i for i in scoring_df.index if i not in train_read_list]\n",
    "    scoring_df = scoring_df.loc[unread_list,:].sort_values('score', ascending=False)\n",
    "    recommendation_list = scoring_df.index[0:top_k]\n",
    "    evaluation_result[user] = recommendation_evaluation(recommendation_list, set(read_list))\n",
    "    count+=1\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", end='\\r')\n",
    "    print(f\"{count/member_list.shape[0]*100}%\", end='\\r')\n",
    "    \n",
    "# convert evaluation_result into dataframe and calculate precision@k\n",
    "lfm_result=pd.DataFrame.from_dict(evaluation_result, orient='index', columns=['result_lfm', 'no_titles_read', 'no_match'])\n",
    "lfm_result['precision@k']=lfm_result['no_match']/top_k\n",
    "\n",
    "# Average accuracy of basic similarity matrix\n",
    "results[result_name]['test accuracy'] = lfm_result['result_lfm'].sum()/lfm_result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22855e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_test_lfm_result = lfm_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_test_lfm_result.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693b0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the results of the models, all 5 models did not fare as well as the basic similarity matrix and the item based collaborative filtering.\n",
    "# Accuracy for BSM on test data set: 0.344784\n",
    "# Accuracy for item based CF similarity matrix on test data set: 0.676590\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abd4ca",
   "metadata": {},
   "source": [
    "### Conclusion for LightFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3ad32",
   "metadata": {},
   "source": [
    "When using LightFM, 5 different models were tried with various features. However, despite trying various features, the accuracy of the basic model, without any feature, performed the best out of the 5 models. This could be due to the number of embedding is too small to learn the features hence resulted in poorer accuracy. However due to time contraint to present the benefits of a recommender system to the client,  as well as the best performing model did not outperform the bsm-cfsm hybrid model, this model is not used due to the time needed to train and optimize the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
